{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":96480,"databundleVersionId":11466546,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:41:28.550169Z","iopub.execute_input":"2025-03-31T08:41:28.550441Z","iopub.status.idle":"2025-03-31T08:42:16.613171Z","shell.execute_reply.started":"2025-03-31T08:41:28.550419Z","shell.execute_reply":"2025-03-31T08:42:16.612346Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# U-Net Inspired Model\n#def build_model(input_shape=(128, 128, 3)):\n    #inputs = Input(input_shape)\n    #c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n    #p1 = MaxPooling2D((2, 2))(c1)\n    #c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n    #p2 = MaxPooling2D((2, 2))(c2)\n    #c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n    #u1 = UpSampling2D((2, 2))(c3)\n    #c4 = Conv2D(128, (3, 3), activation='relu', padding='same')(u1)\n    #u2 = UpSampling2D((2, 2))(c4)\n    #c5 = Conv2D(64, (3, 3), activation='relu', padding='same')(u2)\n    #outputs = Conv2D(1, (1, 1), activation='linear', padding='same')(c5)\n    #return Model(inputs, outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:42:49.721434Z","iopub.execute_input":"2025-03-31T08:42:49.721926Z","iopub.status.idle":"2025-03-31T08:42:49.725291Z","shell.execute_reply.started":"2025-03-31T08:42:49.721899Z","shell.execute_reply":"2025-03-31T08:42:49.724613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\n# RMSE Metric\ndef rmse(y_true, y_pred):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n\n\n\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n\ndef build_model(input_shape=(128, 128, 3)):\n    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n    base_model.trainable = False  # Freeze pretrained layers\n    \n    x = GlobalAveragePooling2D()(base_model.output)\n    x = Dense(512, activation='relu')(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dense(128 * 128, activation='linear')(x)  # Predict depth directly\n    outputs = tf.keras.layers.Reshape((128, 128, 1))(x)\n    \n    return Model(base_model.input, outputs)\n\n# Data generator\ndef data_generator(rgb_dir, depth_dir, batch_size=32):\n    files = sorted(os.listdir(rgb_dir))\n    while True:\n        for i in range(0, len(files), batch_size):\n            batch_files = files[i:i+batch_size]\n            imgs, depths = [], []\n            for file in batch_files:\n                img = cv2.imread(os.path.join(rgb_dir, file), cv2.IMREAD_COLOR)\n                img = cv2.resize(img, (128, 128)) / 255.0\n                depth = cv2.imread(os.path.join(depth_dir, file), cv2.IMREAD_UNCHANGED)\n                depth = cv2.resize(depth, (128, 128)) / 255.0\n                depth = np.expand_dims(depth, axis=-1)\n                imgs.append(img)\n                depths.append(depth)\n            yield np.array(imgs), np.array(depths)\n\n# Load test images\ndef load_test_images(test_dir):\n    images, filenames = [], []\n    for file in sorted(os.listdir(test_dir)):\n        img = cv2.imread(os.path.join(test_dir, file), cv2.IMREAD_COLOR)\n        img = cv2.resize(img, (128, 128)) / 255.0\n        images.append(img)\n        filenames.append(file)\n    return np.array(images), filenames\n\n# Save predictions as images\ndef save_predictions(predictions, filenames, output_folder):\n    os.makedirs(output_folder, exist_ok=True)\n    for pred, fname in zip(predictions, filenames):\n        pred = pred.squeeze()  # Remove channel dimension\n        pred_norm = (pred - np.min(pred)) / (np.max(pred) - np.min(pred) + 1e-6)\n        pred_img = np.uint8(pred_norm * 255)\n        cv2.imwrite(os.path.join(output_folder, fname), pred_img)\n\n# Create simplified CSV in the format shown\ndef create_simplified_csv(predictions, filenames, output_csv):\n    # Prepare data with just 125 dummy columns (based on the example)\n    data = []\n    \n    for idx, (pred, fname) in enumerate(zip(predictions, filenames)):\n        # Just create a row with zeros for all columns (as shown in the example)\n        row = [idx, fname] + [0] * 125  # 125 zeros based on the sample\n        data.append(row)\n    \n    # Create column names: id, ImageID, and numbers 0-124\n    column_names = [\"id\", \"ImageID\"] + list(range(125))\n    \n    # Create DataFrame and save to CSV\n    df = pd.DataFrame(data, columns=column_names)\n    df.to_csv(output_csv, index=False)\n    print(f\"Simplified CSV created at {output_csv} with shape {df.shape}\")\n\n# Paths (adjust accordingly)\ntrain_rgb, train_depth = '/kaggle/input/depth-estimation/competition-data/competition-data/training/images', '/kaggle/input/depth-estimation/competition-data/competition-data/training/depths'\nval_rgb, val_depth = '/kaggle/input/depth-estimation/competition-data/competition-data/validation/images', '/kaggle/input/depth-estimation/competition-data/competition-data/validation/depths'\ntest_rgb = '/kaggle/input/depth-estimation/competition-data/competition-data/testing/images'\noutput_folder = '/kaggle/working/predictions'\noutput_csv = '/kaggle/working/predictions.csv'\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n\ncallbacks = [\n    ModelCheckpoint('model.keras', monitor='val_rmse', save_best_only=True, mode='min'),\n    EarlyStopping(monitor='val_rmse', patience=5, mode='min'),\n    ReduceLROnPlateau(monitor='val_rmse', factor=0.5, patience=3, mode='min')\n]\n\nimport scipy.ndimage\n\ndef refine_depth(predictions):\n    refined_preds = []\n    for pred in predictions:\n        pred = pred.squeeze()  # Remove extra dimension\n        pred_smoothed = scipy.ndimage.gaussian_filter(pred, sigma=1)  # Apply Gaussian blur\n        refined_preds.append(np.expand_dims(pred_smoothed, axis=-1))  # Restore channel dimension\n    return np.array(refined_preds)\n\n# Build & Compile Model\nmodel = build_model()\nmodel.compile(optimizer=Adam(1e-4), loss='mse', metrics=[rmse])\n\n# Train Model\nmodel.fit(\n    data_generator(train_rgb, train_depth),\n    epochs=50,\n    steps_per_epoch=300,\n    validation_data=data_generator(val_rgb, val_depth),\n    validation_steps=50,\n    callbacks=callbacks\n)\n\n# Load Best Model & Predict\nmodel.load_weights('model.keras')\ntest_images, filenames = load_test_images(test_rgb)\npredictions = model.predict(test_images)\npredictions = refine_depth(predictions)  # Apply smoothing\n\n# Save predictions & CSV\nsave_predictions(predictions, filenames, output_folder)\ncreate_simplified_csv(predictions, filenames, output_csv)\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:43:11.173478Z","iopub.execute_input":"2025-03-31T08:43:11.173793Z","iopub.status.idle":"2025-03-31T09:05:21.359210Z","shell.execute_reply.started":"2025-03-31T08:43:11.173769Z","shell.execute_reply":"2025-03-31T09:05:21.358452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training\n#model = build_model()\n#model.compile(optimizer=Adam(), loss='mse', metrics=[rmse])\n#model.fit(\n    #data_generator(train_rgb, train_depth),\n    #epochs=20,\n    #steps_per_epoch=200,\n    #validation_data=data_generator(val_rgb, val_depth),\n    #validation_steps=30,\n    #callbacks=[\n        #ModelCheckpoint('model.keras', monitor='val_rmse', save_best_only=True, mode='min'),\n        #EarlyStopping(monitor='val_rmse', patience=5, mode='min')\n    #]\n#)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prediction\n#model.load_weights('model.keras')\n#test_images, filenames = load_test_images(test_rgb)\n#predictions = model.predict(test_images)\n\n# Save image predictions\n#save_predictions(predictions, filenames, output_folder)\n\n# Create the simplified CSV with the exact format shown\n#create_simplified_csv(predictions, filenames, output_csv)\n\n#print(\"Prediction and CSV generation complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}